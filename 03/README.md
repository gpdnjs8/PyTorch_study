# 03. 머신 러닝 입문하기
## 03-01 **선형 회귀와 자동 미분(Linear Regression and Autograd)**

1. 데이터에 대한 이해
    1. 훈련 데이터셋과 테스트 데이터셋
        - 훈련 데이터셋: 예측을 위해 사용하는 데이터
        - 테스트 데이터셋: 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋
    2. 훈련 데이터셋의 구성
        - 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 한다.
        - 입력과 출력을 각기 다른 텐서에 저장(보편적으로 입력은 x, 출력은 y)
2. 가설 수립
    - 선형 회귀: 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일
    - 선형 회귀의 가설(직선의 방정식)
        
        $$
        y = Wx + b 
        $$
        
        $$
        H(x) = Wx + b 
        $$
        
        *W*: 가중치(Weight), 기울기
        
        *b*: 편향(bias), y절편
        
3. 비용 함수에 대한 이해
    - 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)
    - 평균 제곱 오차를 *W*와 *b*에 의한 비용 함수로 정의
        
        ![비용 함수.png](<비용 함수.png>)
        
        - Cost(*W, b*) 값을 최소로 만드는 *W, b*를 구하면 된다.
4. 옵티마이저 - 경사 하강법(Gradient Descent)
    - Optimizer 알고리즘(최적화 알고리즘): 비용함수의 값을 최소로 하는 *W, b*를 찾을 때 사용
    - 학습(training): 최적화 알고리즘을 통해 적절한 W, b를 찾아내는 과정
    - 경사 하강법
        - cost가 최소화가 되는 지점: 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점
        - 비용 함수(Cost function)를 미분하여 현재 *W*에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 *W*의 값을 변경하는 작업을 반복한다. 이 반복 작업에는 현재 *W*에 접선의 기울기(gradient)를 구해 특정 숫자 *α*를 곱한 값을 빼서 새로운 *W*로 사용하는 식 사용
        
        ![경사 하강법.png](<경사 하강법.png>)
        
        - 기울기가 음수일 때(Negative gradient): *W* 값 증가
            
            ![기울기음수.png](<기울기음수.png>)
            
        - 기울기가 양수일 때(Pogitive gradient): *W* 값 감소
            
            ![기울기양수.png](<기울기양수.png>)
            
            - 결과적으로 기울기가 0인 방향으로 *W* 값 조정
        - 접선의 기울기가 음수거나 양수일 때 모두 접선의 기울기가 0인 방향으로 *W* 값 조정
            
            ![기울기0.png](<기울기0.png>)
            
        - 학습률 *α: W*의 값을 변경할 때, 얼마나 크게 변경할 지 결정
        - 에포크(Epoch): 전체 훈련 데이터가 학습에 한 번 사용된 주기
5. optimizer.zero_grad()
    - 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있다. 따라서 미분값을 계속 0으로 초기화시켜야 한다.
6. torch.manual_seed()
    - 난수 발생 순서와 값을 동일하게 보장해준다. 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있게 해준다.
7. 자동 미분(Autograd)
    - 텐서에는 requires_grad라는 속성이 있다.
        - True로 설정하면 자동 미분 기능 적용
        - 선형 회귀부터 신경망과 같은 복잡한 구조에서 파라미터들이 모두 이 기능 적용
        - requires_grad = True가 적용된 텐서에 연산을 하면, 계산 그래프 생성
        - backward 함수를 호출하면 그래프로부터 자동으로 미분 계산

## 03-02 **다중 선형 회귀**

- 다수의 *x*로부터 *y* 예측

$$
H(x) = XW + B
$$

- *X*: 독립 변수 x들을 (샘플의 수 x 특성의 수)의 크기를 가지는 하나의 행렬
- *W*: 가중치 벡터
- *B*: 편향 벡터

## 03-03 **nn.Module과 클래스로 구현하기**

- 선형 회귀 모델: nn.Linear()
- 평균 제곱오차: nn.functional.mse_loss()
- 클래스로 구현
    - 단일 선형 회귀
        
        ```python
        class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스
            def __init__(self): #
                super().__init__()
                self.linear = nn.Linear(1, 1) # 단순 선형 회귀이므로 input_dim=1, output_dim=1.
        
            def forward(self, x):
                return self.linear(x)
        
        model = LinearRegressionModel()
        ```
        
    - 다중 선형 회귀
        
        ```python
        class MultivariateLinearRegressionModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.
        
            def forward(self, x):
                return self.linear(x)
                
        model = MultivariateLinearRegressionModel()
        ```
        

## 03-04 **미니 배치와 데이터 로더**

1. 미니 배치와 배치 크기
    - 미니 배치
        - 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습
        - 미니 배치 학습 시, 미니 배치만큼만 가져가서 미니 배치에 대한 비용을 계산하고 경사 하강법 수행. 그리고 다음 미니 배치를 가져가서 경사 하강법 수행하고 마지막 미니 배치까지 반복 → 전체 데이터에 대한 학습이 1회 끝나면 1 에포크(Epoch) 끝남
    - 배치 크기
        - 미니 배치의 크기
        - 보통 2의 제곱수 사용
    - 배치 경사 하강법
        - 전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법
        - 가중치 값이 최적값에 수렴하는 과정이 매우 안정적
        - 계산량 많음
    - 미니 배치 경사 하강법
        - 미니 배치 단위로 경사 하강법을 수행하는 방법
        - 최적값으로 수렴하는 과정 힘듦
        - 훈련 속도 빠름
2. 이터레이션
    - 이터레이션
        
        ![이터레이션.png](<이터레이션.png>)
        
        - 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 *W*와 *b*의 업데이트 횟수
        - ex) 전체 데이터 2000, 배치 크기 200이라면 이터레이션 10 → 한 번의 에포크 당 매개변수 업데이트 10번 이루어진다는 뜻
3. 데이터 로드하기
    - 데이터셋, 데이터로더를 사용해 미니 배치 학습, 데이터 셔플, 병렬 처리 수행 가능
    - torch.utils.data.Dataset, torch.utils.data.DataLoade
4. 커스텀 데이터셋
    - torch.utils.data.Dataset을 상속받아 직접 커스텀 데이터를 만들 수 있다.
        - torch.utils.data.Dataset은 파이토치에서 데이터셋을 제공하는 추상 클래스로 Dataset을 상속받아 메소드들을 오버라이드하여 커스텀 데이터셋을 만들 수 있다.
        
        ```python
        class CustomDataset(torch.utils.data.Dataset): 
          def __init__(self):
          데이터셋의 전처리를 해주는 부분
        
          def __len__(self):
          데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분
        
          def __getitem__(self, idx): 
          데이터셋에서 특정 1개의 샘플을 가져오는 함수
        ```
        
5. 커스텀 데이터셋으로 선형 회귀 구현
    - 코드로 실습

## 03-05 **벡터와 행렬 연산 복습하기**

1. 벡터와 행렬과 텐서
    - 벡터: 크기와 방향을 가짐, 1차원 배열
    - 행렬: 행과 열을 가지는 2차원 형상, 2차원 배열
    - 텐서: 3차원 이상의 배열
2. 텐서
    - ndim 출력값: 텐서의 차원 또는 축의 개수
    - shape 출력값: 텐서의 크기(각 축을 따라서 얼마나 많은 차원이 있는지 나타낸 값)
    - 0차원 텐서(스칼라)
        - 하나의 실수값으로 이루어진 데이터, 0D 텐서
    - 1차원 텐서(벡터)
        - 숫자로 배열한 것, 1D 텐서
        - 벡터에서의 차원: 하나의 축에 놓은 원소의 개수 / 텐서에서의 차원: 축의 개수
    - 2차원 텐서(행렬)
        - 행과 열이 존재하는 벡터의 배열, 2D 텐서
    - 3차원 텐서
        - 3D 텐서, 시퀀스 데이터를 표현할 때 자주 사용
            - 시퀀스 데이터: 주로 단어의 시퀀스를 의미하며, 시퀀스는 주로 문장, 문서 등의 텍스트
        - 샘플의 개수: samples, batch_size
        - 시퀀스 길이: timesteps
        - 단어를 표현하는 벡터의 차원: word_dim
    - 그 이상의 텐서
        
        ![다차원텐서.png](<다차원텐서.png>)
        
3. 벡터와 행렬의 연산
    - 코드 실습
4. 다중 선형 회귀 행렬 연산
    
    $$
    H(x) = WX + B
    $$
    
    - 코드 실습
5. 샘플과 특성
    
    ![샘플과특성.png](<샘플과특성.png>)
    
    - *X*: 훈련 데이터의 입력 행렬
    - 샘플: 데이터를 셀 수 있는 단위로 구분했을 때, 각각의 단위
    - 특성: 종속 변수 *y*를 예측하기 위한 각각의 독립 변수 *x*
6. 가중치와 편향 행렬의 크기 설정
    - 행렬 곱셈의 두 가지 주요한 조건
        - 두 행렬의 곱 J × K이 성립되기 위해서는 행렬 J의 열의 개수와 행렬 K의 행의 개수는 같아야 한다.
        - 두 행렬의 곱 J × K의 결과로 나온 행렬 JK의 크기는 J의 행의 개수와 K의 열의 개수를 가진다.
    - 입력과 출력의 행렬의 크기로부터 가중치 행렬 *W*와 편향 행렬 *B*의 크기를 찾을 수 있다.
    - 독립 변수 행렬을 *X*, 종속 변수 행렬을 *Y*라고 하였을 때, 이때 행렬 *X*를 입력 행렬, Y를 출력 행렬이라 한다면
        
        ![행렬.png](<행렬.png>)
        
    - 어떤 딥러닝 모델의 총 매개변수의 개수: 해당 모델에 존재하는 가중치 행렬과 편향 행렬의 모든 원소의 수
