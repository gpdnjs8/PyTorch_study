## 03-01 선형 회귀와 자동 미분**(Linear Regression and Autograd)**

1. 데이터에 대한 이해
    1. 훈련 데이터셋과 테스트 데이터셋
        - 훈련 데이터셋: 예측을 위해 사용하는 데이터
        - 테스트 데이터셋: 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋
    2. 훈련 데이터셋의 구성
        - 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 한다.
        - 입력과 출력을 각기 다른 텐서에 저장(보편적으로 입력은 x, 출력은 y)
2. 가설 수립
    - 선형 회귀: 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일
    - 선형 회귀의 가설(직선의 방정식)
        
        $$
        y = Wx + b 
        $$
        
        $$
        H(x) = Wx + b 
        $$
        
        *W*: 가중치(Weight), 기울기
        
        *b*: 편향(bias), y절편
        
3. 비용 함수에 대한 이해
    - 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)
    - 평균 제곱 오차를 *W*와 *b*에 의한 비용 함수로 정의
        
        ![비용 함수.png](<비용 함수.png>)
        
        - Cost(*W, b*) 값을 최소로 만드는 *W, b*를 구하면 된다.
4. 옵티마이저 - 경사 하강법(Gradient Descent)
    - Optimizer 알고리즘(최적화 알고리즘): 비용함수의 값을 최소로 하는 *W, b*를 찾을 때 사용
    - 학습(training): 최적화 알고리즘을 통해 적절한 W, b를 찾아내는 과정
    - 경사 하강법
        - cost가 최소화가 되는 지점: 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점
        - 비용 함수(Cost function)를 미분하여 현재 *W*에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 *W*의 값을 변경하는 작업을 반복한다. 이 반복 작업에는 현재 *W*에 접선의 기울기(gradient)를 구해 특정 숫자 *α*를 곱한 값을 빼서 새로운 *W*로 사용하는 식 사용
        
        ![경사 하강법.png](<경사 하강법.png>)
        
        - 기울기가 음수일 때(Negative gradient): *W* 값 증가
            
            ![기울기음수.png](<기울기음수.png>)
            
        - 기울기가 양수일 때(Pogitive gradient): *W* 값 감소
            
            ![기울기양수.png](<기울기양수.png>)
            
            - 결과적으로 기울기가 0인 방향으로 *W* 값 조정
        - 접선의 기울기가 음수거나 양수일 때 모두 접선의 기울기가 0인 방향으로 *W* 값 조정
            
            ![기울기0.png](<기울기0.png>)
            
        - 학습률 *α: W*의 값을 변경할 때, 얼마나 크게 변경할 지 결정
        - 에포크(Epoch): 전체 훈련 데이터가 학습에 한 번 사용된 주기
5. optimizer.zero_grad()
    - 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있다. 따라서 미분값을 계속 0으로 초기화시켜야 한다.
6. torch.manual_seed()
    - 난수 발생 순서와 값을 동일하게 보장해준다. 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있게 해준다.
7. 자동 미분(Autograd)
    - 텐서에는 requires_grad라는 속성이 있다.
        - True로 설정하면 자동 미분 기능 적용
        - 선형 회귀부터 신경망과 같은 복잡한 구조에서 파라미터들이 모두 이 기능 적용
        - requires_grad = True가 적용된 텐서에 연산을 하면, 계산 그래프 생성
        - backward 함수를 호출하면 그래프로부터 자동으로 미분 계산
