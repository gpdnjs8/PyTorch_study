# 04. 로지스틱 회귀

## **04-01 로지스틱 회귀(Logistic Regression)**

**로지스틱 회귀: 이진 분류를 풀기 위한 대표적인 알고리즘**

- **이진 분류(Binary Classification): 둘 중 하나를 결정하는 문제**
- **분류 작업에서도 사용 가능**

1. 이진 분류
    - 로지스틱 회귀의 가설은 S자 모양의 그래프를 만들 수 있는 어떤 특정 함수 *f*(시그모이드 함수)를 추가적으로 사용하여 아래와 같은 가설을 사용
    
    $$
    H(x) = f(Wx + b)
    $$
    
2. 시그모이드 함수
    
    $$
    H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)
    $$
    
    - 시그모이드 함수의 출력값은 0과 1 사이의 값을 가진다.
    1. *x*
        
        ![1.png](<1.png>)
        
        - *x*가 매우 커지면 1에 수렴, *x*가 매우 작아지면 0에 수렴
    2. *W*
        
        ![2.png](<2.png>)
        
        - 그래프 경사도 결정, *W* 값 커지면 경사 커지고 *W* 값 작아지면 경사 작아짐
    3. *b*
        
        ![3.png](<3.png>)
        
        - 좌우 이동
3. 비용 함수 
    - 시그모이드 함수
        
        함수의 출력값이 0과 1사이의 값. 즉, 실제값이 1일 때 예측값이 0에 가까워지면 오차가 커져야 하며, 실제값이 0일 때, 예측값이 1에 가까워지면 오차가 커져야 한다. → 로그함수
        
    
    $$
    \text{cost}\left( H(x), y \right) = -[ylogH(x) + (1-y)log(1-H(x))]
    $$
    
    $$
    cost(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]
    $$
    
    $$
    W := W - α\frac{∂}{∂W}cost(W)
    $$
    
4. 파이토치로 구현하기: 코드 실습!

## **04-02 nn.Module과 클래스로 구현하는 로지스틱 회귀**

1. 파이토치의 nn.Linera와 nn.Sigmod로 로지스틱 회귀 구현하기: 코드 실습!
2. 인공 신경망으로 표현되는 로지스틱 회귀
    
    ![인공신경망.png](<인공신경망.png>)
    
    $$
    H(x)=sigmoid(x_{1}w_{1} + x_{2}w_{2} + b)
    $$
    
    - 시그모이드 함수는 인공 신경망의 은닉층에서는 거의 사용되지 않는다.
3. 모델을 클래스로 구현하기: 코드 실습!
    - forward 연산: *H(x)* 식에 입력 *x*로부터 예측된 *y*를 얻는 것
4. 로지스틱 회귀 클래스로 구현하기: 코드 실습!
