# 05. 소프트맥스 회귀

<aside>
🐬

### **소프트맥스 회귀**

**3개 이상의 선택지로부터 1개를 선택하는 문제인 다중 클래스 분류 수행**

</aside>

## **05-01 원-핫 인코딩**

1. 원-핫 인코딩
    - **범주형 데이터를 처리할 때 레이블을 표현하는 방법**
    - 분류 문제 모든 클래스 간의 관계를 균등하게 분배(각 클래스 간 제곱 오차 균등)
2. 원-핫 벡터의 무작위성
    - 원-핫 벡터: 원-핫 인코딩으로 표현된 벡터
    - 원-핫 벡터들은 모든 쌍에 대해서 유클리드 거리를 구해도 전부 유클리드 거리 동일
    - 단어의 유사성을 구할 수 없다.

## **05-02 소프트맥스 회귀 이해하기**

1. 다중 클래스 분류
    - 세 개 이상의 답 중 하나를 고르는 문제
    - 소프트맥스 회귀
        - 소프트맥스 회귀는 확률의 총 합이 1이 되는 아이디어를 다중 클래스 분류 문제에 적용
        - 각 선택지마다 소수 확률 할당, 총 확률의 합 1
        
        $$
        H(X) = softmax(WX + B)
        $$
        
2. 소프트맥스 함수
    
    ![소프트맥스함수.png](<소프트맥스함수.png>)
    
    ![소프트맥스함수2.png](<소프트맥스함수2.png>)
    
    - k차원의 벡터를 입력 받아 각 클래스에 대한 확률 추정(k = 분류해야 하는 클래스의 총 개수)
    - 분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴
    - 샘플 데이터 벡터를 소프트맥스 함수의 입력 벡터로 차원 축소하는 방법
        - 소프트맥스 함수의 입력 벡터 z의 차원 수만큼 결과값이 나오도록 가중치곱 진행
    - 오차 계산 방법
        - 비용 함수로 크로스 엔트로피 함수 사용
3. 비용 함수
    - 크로스 엔트로피 함수
        
        ![비용함수.png](<비용함수.png>)
        
        y: 실제값, k: 클래스 개수, p: 샘플 데이터가 해당 클래스일 확률
        
    - 이진 분류에서의 크로스 엔트로피 함수
        
        ![비용함수2.png](<비용함수2.png>)
        

## **05-03 소프트맥스 회귀 다양한 방법으로 구현하기**

코드 실습! (로우 레벨, 하이 레벨, nn.Module, 클래스를 사용한 방법)

## **05-04 소프트맥스 회귀로 MNIST 데이터 분류하기**

1. MNIST 데이터 이해하기
    
    ![MNIST.png](<MNIST.png>)
    
    - MNIST
        - 숫자 0부터 9까지의 이미지로 구성된 손글씨 데이터셋
        - 총 60,000개의 훈련 데이터와 레이블, 총 10,000개의 테스트 데이터와 레이블로 구성
        - 레이블은 0부터 9까지, 총 10개
    - MNIST 문제
        - 손글씨로 적힌 숫자 이미지가 들어오면, 그 이미지가 무슨 숫자인지 맞추는 문제
2. 토치비전 소개하기
    - 토치비전(torchvision): 유명한 데이터셋들, 이미 구현되어져 있는 유명한 모델들, 일반적인 이미지 전처리 도구들을 포함하고 있는 패키지
3. 분류기 구현을 위한 사전 설정: 코드 실습!
4. MNIST 분류기 구현하기: 코드 실습!
